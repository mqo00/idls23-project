{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Trying to replicate the training procedural of this paper:\n",
        "Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., & Chen, W. (2021). What Makes Good In-Context Examples for GPT-$3 $?. arXiv preprint arXiv:2101.06804.\n",
        "Code repo: https://github.com/jiachangliu/KATEGPT3 "
      ],
      "metadata": {
        "id": "n4RU_Rs0_oG5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZpGRLE9_ISr",
        "outputId": "658f5c4d-e368-40a1-87a9-925ef4033563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence_transformers transformers"
      ],
      "metadata": {
        "id": "DDN-qvHSApUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531257e3-6834-4e82-d4b3-943f03c4c2b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/86.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import torch\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import pairwise\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForMaskedLM\n",
        "\n",
        "#########################################################################################################################################\n",
        "print(\"Is CUDA available? \", torch.cuda.is_available())\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Io6_gWLuAJSm",
        "outputId": "5cf4238e-3198-4b68-9b92-6ca3dff44663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is CUDA available?  True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"/content/gdrive/MyDrive/Sem2/IDL\"\n",
        "# !cd \"/content/gdrive/MyDrive/Colab Notebooks/project/\"\n",
        "!ls\n",
        "\n",
        "# https://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks\n",
        "def chunks(lst, n):\n",
        "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
        "    return [lst[i:i + n] for i in range(0, len(lst), n)]"
      ],
      "metadata": {
        "id": "c5XmHzGa_YuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e3c5772-2d4b-4f22-e740-d74642fa10bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gdrive\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir(\"/content/gdrive/MyDrive/Sem2/IDL/ret_data_update/train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fGkR3Fn-aht",
        "outputId": "fe69d838-8d91-478a-a2e0-8e3637c1ae74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['other_ret_train.tsv',\n",
              " 'lectures_ret_train.tsv',\n",
              " 'logistics_ret_train.tsv',\n",
              " 'hw0_ret_train.tsv',\n",
              " 'project_ret_train.tsv',\n",
              " 'hw1p2_ret_train.tsv',\n",
              " 'quizzes_ret_train.tsv',\n",
              " 'hw1p1_ret_train.tsv',\n",
              " 'hw2p2_ret_train.tsv',\n",
              " 'hw2p1_ret_train.tsv',\n",
              " 'hw3p1_ret_train.tsv',\n",
              " 'hw4p1_ret_train.tsv',\n",
              " 'hw3p2_ret_train.tsv',\n",
              " 'hw4p2_ret_train.tsv',\n",
              " 'quiz_ret_train.tsv',\n",
              " 'hw2p2-s1_ret_train.tsv',\n",
              " 'hw1_ret_train.tsv',\n",
              " 'hw2_ret_train.tsv',\n",
              " 'hw3_ret_train.tsv',\n",
              " 'hw4_ret_train.tsv']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = ['other', 'lectures', 'logistics', 'hw0', 'project', 'hw1p2', 'quizzes', 'hw1p1', 'hw2p2', \n",
        "         'hw2p2', 'hw3p1', 'hw4p1', 'hw3p2', 'hw4p2', 'quiz', 'hw2p2-s1', 'hw1', 'hw2', 'hw3', 'hw4']"
      ],
      "metadata": {
        "id": "YwKQLs_eAGie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = ['hw3p1', 'hw4p1', 'hw3p2', 'hw4p2', 'quiz', 'hw2p2-s1', 'hw1', 'hw2', 'hw3', 'hw4']"
      ],
      "metadata": {
        "id": "iOikbO4xDeXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(files) == len(os.listdir(\"/content/gdrive/MyDrive/Sem2/IDL/ret_data_update/test\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVZZ-8v7AlBJ",
        "outputId": "eb74850b-fbc6-457e-e874-550807581592"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir(\"/content/gdrive/MyDrive/Sem2/IDL/ret_data_update/test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imOnkf5R_veW",
        "outputId": "dd1ba3ed-f4f7-41c0-cfec-47f9f32f6cde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['other_ret_test.tsv',\n",
              " 'lectures_ret_test.tsv',\n",
              " 'logistics_ret_test.tsv',\n",
              " 'hw0_ret_test.tsv',\n",
              " 'project_ret_test.tsv',\n",
              " 'hw1p2_ret_test.tsv',\n",
              " 'quizzes_ret_test.tsv',\n",
              " 'hw1p1_ret_test.tsv',\n",
              " 'hw2p2_ret_test.tsv',\n",
              " 'hw2p1_ret_test.tsv',\n",
              " 'hw3p1_ret_test.tsv',\n",
              " 'hw3p2_ret_test.tsv',\n",
              " 'hw4p1_ret_test.tsv',\n",
              " 'hw4p2_ret_test.tsv',\n",
              " 'quiz_ret_test.tsv',\n",
              " 'hw2p2-s1_ret_test.tsv',\n",
              " 'hw1_ret_test.tsv',\n",
              " 'hw2_ret_test.tsv',\n",
              " 'hw3_ret_test.tsv',\n",
              " 'hw4_ret_test.tsv']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Collective Function"
      ],
      "metadata": {
        "id": "0A3SVfUL8azi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "def decode(tok, model, corpus):\n",
        "    embeddings = []\n",
        "    \n",
        "    if encoder_name == 'roberta-base' or encoder_name == 'roberta-large':\n",
        "        print(\"Using non Sentence Transformer models\")\n",
        "        for corpus_tmp in tqdm(chunks(corpus, 32)):\n",
        "            encoding = tok.batch_encode_plus(corpus_tmp, padding=True, truncation=True)\n",
        "            sentence_batch, attn_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "            sentence_batch, attn_mask = torch.LongTensor(sentence_batch).to(device), torch.LongTensor(attn_mask).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                embedding_output_batch = model(sentence_batch, attn_mask)            \n",
        "                if embed_type == 'mean':\n",
        "                    sentence_embeddings = mean_pooling( embedding_output_batch, attn_mask)\n",
        "                elif embed_type == 'CLS':\n",
        "                    sentence_embeddings = embedding_output_batch[0][:, 0, :]\n",
        "            embeddings.append(sentence_embeddings.detach().cpu().numpy())\n",
        "\n",
        "            del sentence_batch, attn_mask, embedding_output_batch\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "    else:\n",
        "        print(\"Using Sentence Transformer models\")\n",
        "        for corpus_tmp in tqdm(chunks(corpus, 32)):\n",
        "            sentence_embeddings = model.encode(corpus_tmp)\n",
        "            embeddings.append(sentence_embeddings)\n",
        "    \n",
        "    return np.concatenate(embeddings, axis=0)\n"
      ],
      "metadata": {
        "id": "MIEI4MbB9dsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_name = \"roberta-large\"\n",
        "HF_cache_dir = dir+\"/huggingface/cached_transformers/\"\n",
        "tok = RobertaTokenizer.from_pretrained(encoder_name, cache_dir=HF_cache_dir)\n",
        "model = RobertaModel.from_pretrained(encoder_name, cache_dir=HF_cache_dir)\n",
        "model.to(device)\n",
        "\n",
        "args = dict(\n",
        "    Q = \"question\",\n",
        "    A = \"answer\",\n",
        "    train_fname = \"hw0_ret\",\n",
        "    dev_fname = \"hw0_ret\",\n",
        "    embed_type = \"mean\", # CLS\n",
        "    metric = \"cosine\", # euclidean\n",
        "    encoder_name = \"roberta-large\", # roberta-base\n",
        "    num_neighbors = 30,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mfd14k74BWpA",
        "outputId": "c361c7f2-98f0-4fd5-b79d-407bee82fff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "U7U5TZN1BZX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = args['metric']\n",
        "embed_type = args['embed_type']\n",
        "encoder_name = args['encoder_name']\n",
        "num_neighbors = args['num_neighbors']"
      ],
      "metadata": {
        "id": "bvzkuSU3B-jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createNeighbors(train_fname, dev_fname):\n",
        "\n",
        "  args['dev_fname'] = dev_fname\n",
        "  args['train_fname'] = train_fname\n",
        "  Q = args['Q']\n",
        "  A = args['A']\n",
        "  metric = args['metric']\n",
        "  task_name = args['dev_fname']#.split(\"_\")[0]\n",
        "  embed_type = args['embed_type']\n",
        "  encoder_name = args['encoder_name']\n",
        "  num_neighbors = args['num_neighbors']\n",
        "\n",
        "  print(\"The training dataset is {}\".format(args['train_fname']))\n",
        "  print(\"The dev dataset is {}\".format(args['dev_fname']))\n",
        "  print(\"The encoder to get {} {} embeddings is {}\".format(embed_type, metric, encoder_name))\n",
        "  print(\"Task name:\", task_name)\n",
        "\n",
        "  train_fname = dir+\"/ret_data_update/train/{}_ret_train.tsv\".format(args['train_fname'])\n",
        "  dev_fname = dir+\"/ret_data_update/test/{}_ret_test.tsv\".format(args['dev_fname'])\n",
        "\n",
        "  # re separator: (?<![\\\\t].)\\\\t\n",
        "  train_df = pd.read_csv(train_fname, sep='(?<![\\\\t].)\\\\t', quotechar='\"', engine='python', header='infer', keep_default_na=False)\n",
        "  train_corpus = train_df.loc[:, Q].to_list()\n",
        "  train_labels = train_df.loc[:, A].to_list()\n",
        "\n",
        "  train_indices = list(range(len(train_corpus)))\n",
        "\n",
        "  train_corpus = [train_corpus[train_index] for train_index in train_indices]\n",
        "  train_labels = [train_labels[train_index] for train_index in train_indices]\n",
        "\n",
        "  dev_df = pd.read_csv(dev_fname, sep='(?<![\\\\t].)\\\\t|\\\\t(?!\\\\\")', quotechar='\"', engine='python', header='infer', keep_default_na=False)\n",
        "  dev_corpus = dev_df.loc[:, Q].to_list()\n",
        "  dev_labels = dev_df.loc[:, A].to_list()\n",
        "  dev_indices = list(range(len(dev_corpus)))\n",
        "\n",
        "  print(len(train_indices), len(dev_indices))\n",
        "\n",
        "  labels = np.asarray(dev_labels + train_labels)\n",
        "  unique_labels = list(set(labels))\n",
        "  dev_indices = [[] for _ in unique_labels]\n",
        "  for i, label in enumerate(labels):\n",
        "      for j, unique_label in enumerate(unique_labels):\n",
        "          if label == unique_label:\n",
        "              dev_indices[j].append(i)\n",
        "              \n",
        "  n_dev = len(dev_labels)\n",
        "  n_train = len(train_indices)\n",
        "\n",
        "  corpus = dev_corpus + train_corpus\n",
        "\n",
        "  # deep learning model\n",
        "  X = decode(tok, model, corpus)\n",
        "  emb_train = X[n_dev:]\n",
        "  emb_dev = X[:n_dev]\n",
        "\n",
        "  if metric == \"euclidean\":\n",
        "      nbrs = NearestNeighbors(n_neighbors=num_neighbors, algorithm='ball_tree', n_jobs=-1).fit(emb_train)\n",
        "      distances, indices = nbrs.kneighbors(emb_dev)\n",
        "  elif metric == \"cosine\":\n",
        "      dist_matrix = pairwise.cosine_similarity(X=emb_dev, Y=emb_train)\n",
        "      values, indices = torch.topk(torch.from_numpy(dist_matrix), k=num_neighbors, dim=-1)\n",
        "      indices = indices.numpy()\n",
        "\n",
        "  train_indices_np = np.asarray(train_indices)\n",
        "  kNN_dev_train = [train_indices_np[indices[i]].reshape(1, -1) for i in range(len(indices))]\n",
        "  kNN_dev_train = np.concatenate(kNN_dev_train, axis=0)\n",
        "  print(kNN_dev_train.shape)\n",
        "\n",
        "  PIK = \"/content/gdrive/MyDrive/Sem2/IDL/fewshot_files/20s_{}_{}_{}_{}.dat\".format(task_name, encoder_name, metric, embed_type) #dir + \"ret_data/20s{}_{}_{}_{}.dat\".format(task_name, encoder_name, metric, embed_type)\n",
        "\n",
        "  data = dict()\n",
        "  data[\"kNN_dev_train\"] = kNN_dev_train\n",
        "\n",
        "  with open(PIK, \"wb\") as f:\n",
        "      pickle.dump(data, f)\n",
        "\n",
        "  print(\"Finish kNN preprocessing!\")"
      ],
      "metadata": {
        "id": "HeUmC_sR8XKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for f in files:\n",
        "  print(f)\n",
        "  createNeighbors(f, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX55z3sEBBLZ",
        "outputId": "0a4f3d8b-c528-4e7d-b301-f4413c554e4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "other\n",
            "The training dataset is other\n",
            "The dev dataset is other\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: other\n",
            "413 104\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:36<00:00,  2.16s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(104, 30)\n",
            "Finish kNN preprocessing!\n",
            "lectures\n",
            "The training dataset is lectures\n",
            "The dev dataset is lectures\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: lectures\n",
            "272 68\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:16<00:00,  1.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(68, 30)\n",
            "Finish kNN preprocessing!\n",
            "logistics\n",
            "The training dataset is logistics\n",
            "The dev dataset is logistics\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: logistics\n",
            "625 157\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25/25 [00:39<00:00,  1.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(157, 30)\n",
            "Finish kNN preprocessing!\n",
            "hw0\n",
            "The training dataset is hw0\n",
            "The dev dataset is hw0\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: hw0\n",
            "336 85\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:37<00:00,  2.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(85, 30)\n",
            "Finish kNN preprocessing!\n",
            "project\n",
            "The training dataset is project\n",
            "The dev dataset is project\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: project\n",
            "440 110\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 18/18 [00:36<00:00,  2.02s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(110, 30)\n",
            "Finish kNN preprocessing!\n",
            "hw1p2\n",
            "The training dataset is hw1p2\n",
            "The dev dataset is hw1p2\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: hw1p2\n",
            "556 140\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [01:04<00:00,  2.92s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(140, 30)\n",
            "Finish kNN preprocessing!\n",
            "quizzes\n",
            "The training dataset is quizzes\n",
            "The dev dataset is quizzes\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: quizzes\n",
            "492 124\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:41<00:00,  2.06s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(124, 30)\n",
            "Finish kNN preprocessing!\n",
            "hw1p1\n",
            "The training dataset is hw1p1\n",
            "The dev dataset is hw1p1\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: hw1p1\n",
            "411 103\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:47<00:00,  2.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(103, 30)\n",
            "Finish kNN preprocessing!\n",
            "hw2p2\n",
            "The training dataset is hw2p2\n",
            "The dev dataset is hw2p2\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: hw2p2\n",
            "412 104\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:47<00:00,  2.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(104, 30)\n",
            "Finish kNN preprocessing!\n",
            "hw2p2\n",
            "The training dataset is hw2p2\n",
            "The dev dataset is hw2p2\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: hw2p2\n",
            "412 104\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17/17 [00:47<00:00,  2.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(104, 30)\n",
            "Finish kNN preprocessing!\n",
            "hw3p1\n",
            "The training dataset is hw3p1\n",
            "The dev dataset is hw3p1\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: hw3p1\n",
            "344 87\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:40<00:00,  2.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(87, 30)\n",
            "Finish kNN preprocessing!\n",
            "hw4p1\n",
            "The training dataset is hw4p1\n",
            "The dev dataset is hw4p1\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: hw4p1\n",
            "224 56\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9/9 [00:26<00:00,  2.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(56, 30)\n",
            "Finish kNN preprocessing!\n",
            "hw3p2\n",
            "The training dataset is hw3p2\n",
            "The dev dataset is hw3p2\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: hw3p2\n",
            "373 94\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:43<00:00,  2.92s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(94, 30)\n",
            "Finish kNN preprocessing!\n",
            "hw4p2\n",
            "The training dataset is hw4p2\n",
            "The dev dataset is hw4p2\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: hw4p2\n",
            "383 96\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [00:45<00:00,  3.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(96, 30)\n",
            "Finish kNN preprocessing!\n",
            "quiz\n",
            "The training dataset is quiz\n",
            "The dev dataset is quiz\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: quiz\n",
            "132 34\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6/6 [00:11<00:00,  1.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(34, 30)\n",
            "Finish kNN preprocessing!\n",
            "hw2p2-s1\n",
            "The training dataset is hw2p2-s1\n",
            "The dev dataset is hw2p2-s1\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: hw2p2-s1\n",
            "117 30\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:14<00:00,  2.81s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(30, 30)\n",
            "Finish kNN preprocessing!\n",
            "hw1\n",
            "The training dataset is hw1\n",
            "The dev dataset is hw1\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: hw1\n",
            "489 123\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:55<00:00,  2.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(123, 30)\n",
            "Finish kNN preprocessing!\n",
            "hw2\n",
            "The training dataset is hw2\n",
            "The dev dataset is hw2\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: hw2\n",
            "584 146\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 23/23 [01:07<00:00,  2.95s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(146, 30)\n",
            "Finish kNN preprocessing!\n",
            "hw3\n",
            "The training dataset is hw3\n",
            "The dev dataset is hw3\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: hw3\n",
            "540 136\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 22/22 [01:02<00:00,  2.86s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(136, 30)\n",
            "Finish kNN preprocessing!\n",
            "hw4\n",
            "The training dataset is hw4\n",
            "The dev dataset is hw4\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: hw4\n",
            "349 88\n",
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14/14 [00:40<00:00,  2.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(88, 30)\n",
            "Finish kNN preprocessing!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Individual Chunks"
      ],
      "metadata": {
        "id": "qExsRbCY8YIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "args = dict(\n",
        "    Q = \"question\",\n",
        "    A = \"answer\",\n",
        "    train_fname = \"hw0_ret\",\n",
        "    dev_fname = \"hw0_ret\",\n",
        "    embed_type = \"mean\", # CLS\n",
        "    metric = \"cosine\", # euclidean\n",
        "    encoder_name = \"roberta-large\", # roberta-base\n",
        "    num_neighbors = 30,\n",
        ")\n",
        "\n",
        "Q = args['Q']\n",
        "A = args['A']\n",
        "metric = args['metric']\n",
        "task_name = args['dev_fname'].split(\"_\")[0]\n",
        "embed_type = args['embed_type']\n",
        "encoder_name = args['encoder_name']\n",
        "num_neighbors = args['num_neighbors']\n",
        "\n",
        "print(\"The training dataset is {}\".format(args['train_fname']))\n",
        "print(\"The dev dataset is {}\".format(args['dev_fname']))\n",
        "print(\"The encoder to get {} {} embeddings is {}\".format(embed_type, metric, encoder_name))\n",
        "print(\"Task name:\", task_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9h-tFePAO7o",
        "outputId": "3a4d97ee-e474-4b27-ed0b-9e57370fa5f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The training dataset is hw0_ret\n",
            "The dev dataset is hw0_ret\n",
            "The encoder to get mean cosine embeddings is roberta-large\n",
            "Task name: hw0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_fname = dir+\"/ret_data_update/train/{}_train.tsv\".format(args['train_fname'])\n",
        "dev_fname = dir+\"/ret_data_update/20s_test/{}_test.tsv\".format(args['dev_fname'])\n",
        "HF_cache_dir = dir+\"/huggingface/cached_transformers/\"\n",
        "\n",
        "# if encoder_name == \"roberta-base\" or \"roberta-large\":\n",
        "tok = RobertaTokenizer.from_pretrained(encoder_name, cache_dir=HF_cache_dir)\n",
        "model = RobertaModel.from_pretrained(encoder_name, cache_dir=HF_cache_dir)\n",
        "\n",
        "# print(\"SentenceTransformer model\")\n",
        "# tok = None\n",
        "# model = SentenceTransformer(\"{}{}\".format(HF_cache_dir, encoder_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldtwoN0hDC7Q",
        "outputId": "8b8458b6-5682-4e1e-c0ed-c59b84c67c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re separator: (?<![\\\\t].)\\\\t\n",
        "train_df = pd.read_csv(train_fname, sep='(?<![\\\\t].)\\\\t', quotechar='\"', engine='python', header='infer', keep_default_na=False)\n",
        "train_corpus = train_df.loc[:, Q].to_list()\n",
        "train_labels = train_df.loc[:, A].to_list()\n",
        "\n",
        "train_indices = list(range(len(train_corpus)))\n",
        "\n",
        "train_corpus = [train_corpus[train_index] for train_index in train_indices]\n",
        "train_labels = [train_labels[train_index] for train_index in train_indices]\n",
        "\n",
        "dev_df = pd.read_csv(dev_fname, sep='(?<![\\\\t].)\\\\t|\\\\t(?!\\\\\")', quotechar='\"', engine='python', header='infer', keep_default_na=False)\n",
        "dev_corpus = dev_df.loc[:, Q].to_list()\n",
        "dev_labels = dev_df.loc[:, A].to_list()\n",
        "dev_indices = list(range(len(dev_corpus)))\n",
        "\n",
        "print(len(train_indices), len(dev_indices))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UdQD_Qw2C4ur",
        "outputId": "e4f6ae99-bb51-4f9f-806c-35355fee0507"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "336 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
        "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    return sum_embeddings / sum_mask\n",
        "\n",
        "def decode(tok, model, corpus):\n",
        "    embeddings = []\n",
        "    \n",
        "    if encoder_name == 'roberta-base' or encoder_name == 'roberta-large':\n",
        "        print(\"Using non Sentence Transformer models\")\n",
        "        for corpus_tmp in tqdm(chunks(corpus, 32)):\n",
        "            encoding = tok.batch_encode_plus(corpus_tmp, padding=True, truncation=True)\n",
        "            sentence_batch, attn_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "            sentence_batch, attn_mask = torch.LongTensor(sentence_batch).to(device), torch.LongTensor(attn_mask).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                embedding_output_batch = model(sentence_batch, attn_mask)            \n",
        "                if embed_type == 'mean':\n",
        "                    sentence_embeddings = mean_pooling( embedding_output_batch, attn_mask)\n",
        "                elif embed_type == 'CLS':\n",
        "                    sentence_embeddings = embedding_output_batch[0][:, 0, :]\n",
        "            embeddings.append(sentence_embeddings.detach().cpu().numpy())\n",
        "\n",
        "            del sentence_batch, attn_mask, embedding_output_batch\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "    else:\n",
        "        print(\"Using Sentence Transformer models\")\n",
        "        for corpus_tmp in tqdm(chunks(corpus, 32)):\n",
        "            sentence_embeddings = model.encode(corpus_tmp)\n",
        "            embeddings.append(sentence_embeddings)\n",
        "    \n",
        "    return np.concatenate(embeddings, axis=0)\n"
      ],
      "metadata": {
        "id": "-2jNxbtoC43D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "labels = np.asarray(dev_labels + train_labels)\n",
        "unique_labels = list(set(labels))\n",
        "dev_indices = [[] for _ in unique_labels]\n",
        "for i, label in enumerate(labels):\n",
        "    for j, unique_label in enumerate(unique_labels):\n",
        "        if label == unique_label:\n",
        "            dev_indices[j].append(i)\n",
        "            \n",
        "n_dev = len(dev_labels)\n",
        "n_train = len(train_indices)\n",
        "\n",
        "corpus = dev_corpus + train_corpus\n",
        "\n",
        "# deep learning model\n",
        "model.to(device)\n",
        "X = decode(tok, model, corpus)\n",
        "emb_train = X[n_dev:]\n",
        "emb_dev = X[:n_dev]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AI7F0uSC5BR",
        "outputId": "25d4a38d-3b3a-425d-d410-c248223f1b29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using non Sentence Transformer models\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [00:29<00:00,  2.65s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if metric == \"euclidean\":\n",
        "    nbrs = NearestNeighbors(n_neighbors=num_neighbors, algorithm='ball_tree', n_jobs=-1).fit(emb_train)\n",
        "    distances, indices = nbrs.kneighbors(emb_dev)\n",
        "elif metric == \"cosine\":\n",
        "    dist_matrix = pairwise.cosine_similarity(X=emb_dev, Y=emb_train)\n",
        "    values, indices = torch.topk(torch.from_numpy(dist_matrix), k=num_neighbors, dim=-1)\n",
        "    indices = indices.numpy()\n",
        "\n",
        "train_indices_np = np.asarray(train_indices)\n",
        "kNN_dev_train = [train_indices_np[indices[i]].reshape(1, -1) for i in range(len(indices))]\n",
        "kNN_dev_train = np.concatenate(kNN_dev_train, axis=0)\n",
        "print(kNN_dev_train.shape)\n",
        "\n",
        "PIK = \"/content/20s{}_{}_{}_{}.dat\".format(task_name, encoder_name, metric, embed_type) #dir + \"ret_data/20s{}_{}_{}_{}.dat\".format(task_name, encoder_name, metric, embed_type)\n",
        "\n",
        "data = dict()\n",
        "data[\"kNN_dev_train\"] = kNN_dev_train\n",
        "\n",
        "with open(PIK, \"wb\") as f:\n",
        "    pickle.dump(data, f)\n",
        "\n",
        "print(\"Finish kNN preprocessing!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqegR8a2C930",
        "outputId": "9f410049-f15c-4252-c07e-b22ee3973580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7, 30)\n",
            "Finish kNN preprocessing!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(data[\"kNN_dev_train\"][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2Ipw_6Nxl_s",
        "outputId": "7c17766c-7c4a-4891-a1a9-354d66f0f32b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[\"kNN_dev_train\"]\n",
        "\n",
        "for i in range(len(data)): # test entries\n",
        "    neighbors = data[i] # top k\n",
        "    # print(neighbors)\n",
        "    print(\"Q\", dev_corpus[i])\n",
        "    print(\"A\", dev_labels[i])\n",
        "    for i in range(len(neighbors)):\n",
        "        index = neighbors[i]\n",
        "        print(f\"Top {i} Q\", train_corpus[index])\n",
        "        print(f\"Top {i} A\", train_labels[index])\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqMHm5N-NMBX",
        "outputId": "20a374d3-3451-498f-87cd-c99cf2931354"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q I have used several ways to implement the function. I think they look right on my notebook, but it just cannot pass Autolab:  answer1:  return torch.where(x&gt;0,torch.ones_like(x),torch.zeros_like(x)) answer2: return torch.gt(x,0).long() Could anyone take a look? Thanks!\n",
            "A When the value is &gt;= 0, it should be 1\n",
            "Top 0 Q I have used several ways to implement the function. I think they look right on my notebook, but it just cannot pass Autolab:  answer1:  return torch.where(x&gt;0,torch.ones_like(x),torch.zeros_like(x)) answer2: return torch.gt(x,0).long() Could anyone take a look? Thanks!\n",
            "Top 0 A When the value is &gt;= 0, it should be 1\n",
            "Top 1 Q For question 2.6 in hw0p1       y = torch.where(x &gt; 0, torch.tensor(1), torch.tensor(0)) This is the code I have written and it has passed the autograder. However, I was wondering if there is a better way to do this instead of using torch.tensor(1)?  Thanks for all the help! \n",
            "Top 1 A torch.tensor(1) or (0) is not required.  When you use the comparison operator just like that you get a boolean array/tensor. You just need to convert that boolean tensor to int/long\n",
            "Top 2 Q \"Hi,  I am facing a memory error when testing NUMPY_sumproduct function on autolab.  The function is defined as given below- def NUMPY_sumproduct(x, y):    return np.outer(X,Y).sum() It executes fine on my windows machine running numpy version 1.16.4 and even produces output(265421520) on autolabHowever I am still seeing the error  ERROR: test_sumproduct (__main__.TestHW0) ---------------------------------------------------------------------- Traceback (most recent call last):   File \"\"/home/autograde/autolab/autograde/grade.py\"\", line 882, in test_sumproduct     student_answer = NUMPY_sumproduct(self.X, self.Y)   File \"\"/home/autograde/autolab/autograde/handin.py\"\", line 583, in NUMPY_sumproduct     return np.outer(X,Y).sum()   File \"\"/usr/local/depot/anaconda3.7/lib/python3.7/site-packages/numpy/core/numeric.py\"\", line 1203, in outer     return multiply(a.ravel()[:, newaxis], b.ravel()[newaxis, :], out) MemoryError Not sure why I am getting this error here?   Also, I am posting this privately as there is a code snippet involved-could you please let me know if this is the right way to do it.\"\n",
            "Top 2 A My guess is that ndarray.sum() is in the one throwing the error.\n",
            "Top 3 Q \"For tensor_RELU &amp; tensor_RELU_prime, we know that Parameters: x (torch.Tensor): 2-dimensional torch tensor. While running on Autolab, I got error messages as below FAIL: test_tensor_ReLU (__main__.TestHW0) ---------------------------------------------------------------------- Traceback (most recent call last):   File \"\"/home/autograde/autolab/autograde/hw0_test.py\"\", line 951, in test_tensor_ReLU     student_answer = tensor_ReLU(self.tensor_X)   File \"\"/home/autograde/autolab/autograde/handin.py\"\", line 1436, in tensor_ReLU     assert(len(x.shape) == 2) AssertionError  Similarly for tensor_RELU_prime. Does this mean input is not 2-dimensional tensor?\"\n",
            "Top 3 A the input is 2-dimensional\n",
            "Top 4 Q Hi!  This is driving me a little crazy, but I don't know why when I use torch.mul, that my dimensions won't line up for PYTORCH_sumproduct. Currently, this is what I have. The shape of t_ones, which is the first transposed matrix of ones, is (1, 3000), then the shape of outer_prod, which is how I wrote the previous outer production function is (3000,3000), then the shape of ones, which is the second matrix of ones, is (3000,1). I keep getting a matrix output that's (3000,3000). I've tried tracking the dimensions but it keeps reverting to (3000,3000). Am I missing a dimensionality change here? Thanks!   t_ones = torch.ones(len(x)) t_ones = torch.reshape(t_ones, (1, len(x))) outer_prod = torch.mul(torch.reshape(x, (len(x), 1)), torch.reshape(y, (1, len(y)))) ones = torch.ones(len(y)) ones = torch.reshape(ones, (len(y), 1)) return torch.mul(t_ones, torch.mul(outer_prod, ones))\n",
            "Top 4 A torch.mul is doing element-wise multiplication, you can refer to here: https://pytorch.org/docs/stable/generated/torch.mul.htmlIf you want to do matrix multiplication, I guess you need to use torch.matmul: https://pytorch.org/docs/stable/generated/torch.matmul.html\n",
            "Top 5 Q Hello, I struggle a lot with the parts 4.1 and 4.2 because the corrector gives me 0 points but I don't understand why in both cases. Is it possible to have some help please? The code for part 1: def numpy2tensor(x):     #return torch.IntTensor(list(x))     t = torch.ones(1,dtype=torch.long) * x[0]     for i in range(x.shape[0]-1):         t = torch.cat((t,torch.ones(1,dtype=torch.long) * x[i+1]),0)     return t The code for 4.2: def tensor2numpy(x):     return np.array(x.tolist()) Thanks in advance for the help\n",
            "Top 5 A Why is your code so complex for converting to numpy to tensor and tensor to numpy? They are both achieved through simple one-line Pytorch functionalities you can find in Pytorch's documentation. https://pytorch.org/docs/stable/index.html\n",
            "Top 6 Q \"Hello,  I am trying to submit my files on Autolab. I have combined all the jupyter notebooks into a .tar.gz file. Upon submission, autolab gives the following error  :  File \"\"/usr/local/depot/anaconda3.7/lib/python3.7/codecs.py\"\", line 322, in decode     (result, consumed) = self._buffer_decode(data, self.errors, final) UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte make: *** [all] Error 1 The error does not point to any particular line in my code. Please guide me on how to proceed.  Thank you, Prerna\"\n",
            "Top 6 A Is this for HW0 Part 2 or Part 1? You submit the P1 and P2 in their respective autolab submission locations. For this homework you should just submit the notebook as is.\n",
            "Top 7 Q \"Hi!  I'm running into a few errors with AutoLab for my submission for part 1. These parts work fine on Jupyter Notebook (with what should be the correct package versions), but not for submission.  1.2: This returns no errors, and the output seems correct:   input: tensor([ 752, -581,  288,  ..., -690, -869, -848]) output: [ 752. -581.  288. ... -690. -869. -848.] However, I got 0 out of 2.5. Is it the way that the output was formatted? This was my code for the function:   def tensor2numpy(x):     \"\"\"\"\"\"     Creates a numpy.ndarray from a torch.Tensor.      Parameters:     x (torch.Tensor): 1-dimensional torch tensor.      Returns:     numpy.ndarray: 1-dimensional numpy array.     \"\"\"\"\"\"      new_x = np.zeros(len(x))     for i in range(len(new_x)):         new_x[i] = x[i]              return new_x 2.5 &amp; 2.6: I'm getting another long and float issue. This was the error from autolabs:   Traceback (most recent call last):   File \"\"/home/autograde/autolab/autograde/grade.py\"\", line 19, in &lt;module&gt;     from handin import *   File \"\"/home/autograde/autolab/autograde/handin.py\"\", line 837, in &lt;module&gt;     print(PYTORCH_PrimeReLU(X))   File \"\"/home/autograde/autolab/autograde/handin.py\"\", line 823, in PYTORCH_PrimeReLU     return ones_arr*(x &gt; zeros_arr) RuntimeError: Expected object of scalar type Long but got scalar type Float for argument #2 'other' make: *** [autograde] Error 1 I tried to convert the arrays into type Long like in my previous post, but then I get this error:   Traceback (most recent call last):   File \"\"/home/autograde/autolab/autograde/grade.py\"\", line 19, in &lt;module&gt;     from handin import *   File \"\"/home/autograde/autolab/autograde/handin.py\"\", line 716, in &lt;module&gt;     print(PYTORCH_ReLU(X))   File \"\"/home/autograde/autolab/autograde/handin.py\"\", line 702, in PYTORCH_ReLU     return x*(x &gt; zeros_arr) RuntimeError: expected device cpu and dtype Long but got device cpu and dtype Bool make: *** [autograde] Error 1 I was using the comparison functions similar to recitation 0, but is this a problem with tensors in particular? Do I have to convert the booleans?\"\n",
            "Top 7 A It looks like you are running into a data type issue. You should convert your variables in response to the errors. For example, to make X be float, you use X.float(); or, to make X be long, you use X.long().When working with pytorch, you will see this similar back and forth when training.\n",
            "Top 8 Q \"Hi, I'm working on the pytorch dot product function and noticed that returning a torch.int64 outputs the following  tensor(7082791) instead of just 7082791, like the expected output. Is this something I need to fix in my code or did the expected output omit \"\"tensor()\"\"? Thank you! \"\n",
            "Top 8 A You may use https://pytorch.org/docs/stable/generated/torch.Tensor.item.html\n",
            "Top 9 Q Hi, as I am working on the hw0p1 2.1, when using torch.dot(x, y) to perform the dot product on 2 1d-tensors, I will eventually a tensor instead of a scalar quantity. I wonder what function can we use in order to covert torch.tensor() to torch.int64?\n",
            "Top 9 A You can use torch.to(torch.int64) to convert the tensor into a torch.int64, and use .item() to the element from a singleton tensor.\n",
            "Top 10 Q For the implementation of relu function in hw0p1,   I was able to use the PyTorch tensors as numpy arrays as in:  x[x&lt;0] = 0  would this be considered as a correct implementation for the relu function?   Thanks in advance\n",
            "Top 10 A Looks good to me. Are you getting an error on AutoLab?\n",
            "Top 11 Q \" In Recitations Print Debugging is shown as(1)- print(\"\"Resut in {}-th iteration:\\n {}\"\".format(i, result))   However, in earlier recitations following format was also used(2)- print(f\"\"Result in {i, result}-th iteration:\\n {i, result}\"\") i.e no use of (.format)  When I am trying to run this code its giving syntax error.(Code 2) \"\n",
            "Top 11 A There should only be one variable inside { } in code 2. So based on the code you have shared, the first { } should have only 'i' and the next one should have 'result'You can google f-strings for more info on this\n",
            "Top 12 Q \"Hello, I tried submitting HW0 Part 1 to autolab but ran into a type error that caused all my problems to cascading fail. Reading the trace, I'm not sure where the error occurred, and the line number doesn't make much sense since we're using notebook.  My only guess rn is that my rectifier in 2.5 is outputting longs instead of floats, but all the values do have the . after it if I print it?   Feedback for hw0p1 - 1.1 (jimmyzen@andrew.cmu.edu) An error occurred while parsing the autoresult returned by the Autograder.           Error message: 783: unexpected token at ': *** [autograde] Error 1'  Autograder [Sun Sep  5 17:07:58 2021]: Received job 11785-f21_hw0p1_3_jimmyzen@andrew.cmu.edu:1033 Autograder [Sun Sep  5 17:08:16 2021]: Success: Autodriver returned normally Autograder [Sun Sep  5 17:08:16 2021]: Here is the output from the autograder: --- Autodriver: Job exited with status 2 tar xvf autograde.tar autograde/ autograde/._grade.py autograde/grade.py cp handin.ipynb autograde /usr/local/depot/anaconda3.7/bin/jupyter nbconvert --to script /home/autograde/autolab/autograde/handin.ipynb [NbConvertApp] Converting notebook /home/autograde/autolab/autograde/handin.ipynb to script [NbConvertApp] Writing 114216 bytes to /home/autograde/autolab/autograde/handin.py /usr/local/depot/anaconda3.7/bin/python3 /home/autograde/autolab/autograde/grade.py 1.16.4 1.2.0 2 1 &lt;class 'torch.Tensor'&gt; &lt;class 'numpy.ndarray'&gt; 7082791 7082791 [[  59092 -144096  136512 ...  -53088  -86268   53404]  [  82467 -201096  190512 ...  -74088 -120393   74529]  [-122111  297768 -282096 ...  109704  178269 -110357]  ...  [-144551  352488 -333936 ...  129864  211029 -130637]  [-179707  438216 -415152 ...  161448  262353 -162409]  [  88825 -216600  205200 ...  -79800 -129675   80275]] tensor([[  59092, -144096,  136512,  ...,  -53088,  -86268,   53404],         [  82467, -201096,  190512,  ...,  -74088, -120393,   74529],         [-122111,  297768, -282096,  ...,  109704,  178269, -110357],         ...,         [-144551,  352488, -333936,  ...,  129864,  211029, -130637],         [-179707,  438216, -415152,  ...,  161448,  262353, -162409],         [  88825, -216600,  205200,  ...,  -79800, -129675,   80275]]) [  59092 -201096 -282096 ...  129864  262353   80275] tensor([  59092, -201096, -282096,  ...,  129864,  262353,   80275]) 265421520 265421520 [[  0   0 653 ... 773 961   0]  [  0 456   0 ... 168 273   0]  [936 475   0 ... 408   0   0]  ...  [  0 396 457 ... 646   0   0]  [645 943   0 ... 863   0 790]  [641   0 379 ... 347   0   0]] Traceback (most recent call last):   File \"\"/home/autograde/autolab/autograde/grade.py\"\", line 19, in &lt;module&gt;     from handin import *   File \"\"/home/autograde/autolab/autograde/handin.py\"\", line 729, in &lt;module&gt;     print(PYTORCH_ReLU(X))   File \"\"/home/autograde/autolab/autograde/handin.py\"\", line 712, in PYTORCH_ReLU     stacked_arr = torch.stack([zero_arr, x], axis = 0) RuntimeError: Expected object of scalar type Float but got scalar type Long for sequence element 1 in sequence argument at position #1 'tensors' make: *** [autograde] Error 1  Score for this problem: 0.0\"\n",
            "Top 12 A It's possible that zero_arr and x have different data types. PyTorch is more picky than numpy that way.\n",
            "Top 13 Q Following is the error message for all the questions:  File “/home/autograde/autolab/autograde/handin.py”, line 709np.random.seed(0)^SyntaxError: invalid syntaxmake: *** [autograde] Error 1Score for this problem: 0.0  Please provide guidance on what should I do here. Thank you!mmalani \n",
            "Top 13 A Strange. Are there any indent errors? How did you run your notebook? locally or Colab?\n",
            "Top 14 Q \"Once I create a new tensor, I will add \"\"dtype=torch.int32\"\" and I will also get a suffix \"\"dtype=torch.int32\"\" in the result. Does it matter or what is the purpose of \"\"dtype=torch.int32\"\"？ \"\n",
            "Top 14 A Each tensor got a dtype, indicating the datatype of the tensor. And the default dtype for int in pytorch is torch.int64, any other int dtype will appear in the output.\n",
            "Top 15 Q https://piazza.com/class/knsmz2b3z131mn?cid=15_f31  Code that causes the error-    I get this error on Autolab, but code works fine when I run locally (expected output matches mine)    Thank you in advance!\n",
            "Top 15 A Check your __getitem__, are you sure you have to return X[i] or something else? See how you have defined it in __init__\n",
            "Top 16 Q \"Hello instructors, I am submitting my codes to autolab which gave me exactly the same answers as provided in the jupyter notebook but I am getting zero. Here is how it looks like on my autolab account     trying to see the error by clicking on the zero, here is what the autolab is telling me::   Feedback for hw0p1 - 1.1 (vitalh@andrew.cmu.edu)  An error occurred while parsing the autoresult returned by the Autograder.           Error message: 859: unexpected token at ': *** [autograde] Error 1'  Autograder [Wed Aug 31 05:21:51 2022]: Received job 11785-f22_hw0p1_1_vitalh@andrew.cmu.edu:1955 Autograder [Wed Aug 31 05:22:09 2022]: Success: Autodriver returned normally Autograder [Wed Aug 31 05:22:09 2022]: Here is the output from the autograder: --- Autodriver: Job exited with status 2 tar xvf autograde.tar autograde/ autograde/._grade.py autograde/grade.py cp handin.ipynb autograde /usr/local/depot/anaconda3.7/bin/jupyter nbconvert --to script /home/autograde/autolab/autograde/handin.ipynb [NbConvertApp] Converting notebook /home/autograde/autolab/autograde/handin.ipynb to script [NbConvertApp] Writing 110012 bytes to /home/autograde/autolab/autograde/handin.py /usr/local/depot/anaconda3.7/bin/python3 /home/autograde/autolab/autograde/grade.py 1.16.4 1.2.0 2 1 2 1 0 &lt;class 'torch.Tensor'&gt; &lt;class 'numpy.ndarray'&gt; 7082791 7082791 [[  59092 -144096  136512 ...  -53088  -86268   53404]  [  82467 -201096  190512 ...  -74088 -120393   74529]  [-122111  297768 -282096 ...  109704  178269 -110357]  ...  [-144551  352488 -333936 ...  129864  211029 -130637]  [-179707  438216 -415152 ...  161448  262353 -162409]  [  88825 -216600  205200 ...  -79800 -129675   80275]] Traceback (most recent call last):   File \"\"/home/autograde/autolab/autograde/grade.py\"\", line 19, in &lt;module&gt;     from handin import *   File \"\"/home/autograde/autolab/autograde/handin.py\"\", line 380, in &lt;module&gt;     print(PYTORCH_outer(X,Y))   File \"\"/home/autograde/autolab/autograde/handin.py\"\", line 362, in PYTORCH_outer     resutls = torch.outer(x,y) AttributeError: module 'torch' has no attribute 'outer' make: *** [autograde] Error 1 Score for this problem: 0.0  Graded by:    \"\n",
            "Top 16 A Did you downgrade torch version? if so dont do that. Some versions wont have outer.\n",
            "Top 17 Q \"I got the error \"\"IndexError: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number\"\" while I was using code like x[i][j].item() (x is 2d tensor). What is the correct way of accessing single element in the tensor?  Thanks.\"\n",
            "Top 17 A Post a screenshot of your code, while I guess the reason is your x is actually an 1-d tensor\n",
            "Top 18 Q Dear instructores,  I tried the code numpy.outer(x,y), and it returns a correct numpy array. x and y are both 1-dimensional torch tensor.  What's the mechanism behind this? Why could numpy function support torch tensor? Are there any related material I can read?\n",
            "Top 18 A Hello,  Check this link out: https://discuss.pytorch.org/t/numpy-operations-on-pytorch-tensor/20737 Also, this is a generic question, please use the HW0 thread to post this @29, as other students will benefit from it. Can I make this post public?\n",
            "Top 19 Q The first run-through of my hw0p2 runs without errors on Autolab, but during the second run-through I get the following error with ExampleDataset2:  ValueError: expected sequence of length 6 at dim 1 (got 4) I've run my notebook locally several times in Google Colab and did not encounter this problem. How can this issue be resolved?\n",
            "Top 19 A What Python version of Autolab are you using? This is one reason for the varying results between Autolab and your own environment. Autolab runs on PyTorch v1.2.0.  We can start by ensuring you Colab is running PyTorch v1.2.0 and see if the error is reproducible? In which case, you would have to find a suitable API method from v1.2.0 to fix your code.  Also and very importantly, it doesn't help to post a single line error without a stack trace and/or your attempts (code snippets or screenshots) leading to the error. It helps us to help you when you are more explicit about the exact issue.  Let me know how it goes switching your Colab's PyTorch version.\n",
            "Top 20 Q Every time I run one of these pre-defined functions I get the following error:  ``` /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray ```  Is there a way to fix this or should we just ignore it? I am using Colab with NumPy version 1.19.5.\n",
            "Top 20 A You can ignore any deprecation warning. You see this warning when a certain function is no longer supported, but you can still use it. You can use the alternative recommended style as well.\n",
            "Top 21 Q \"Hi all,  I got a bit confused over the homework assignment specs here, for example, in section 2.1 dot product, is it fine if we simply return np.dot(x, y)? or are we supposed to do something less direct like np.sum(x * y)? I am unsure of the word \"\"implement\"\" here. Are we allowed to use existing numpy function to solve these problems?  Thank you!\"\n",
            "Top 21 A Similar questions have already been asked on the HW0 FAW thread. You might want to check that out. Moreover:1. Please post generic questions on public threads. 2. Always check the public threads to see if someone has already asked a similar question before. 3. Please check Piazza etiquette to see more guidelines on using Piazza effectively.\n",
            "Top 22 Q \"Hi. I got all the expected output for hw0p2(easy difficulty) but when I submitted to autolab it showed:   An error occurred while parsing the autoresult returned by the Autograder. Error message: 783: unexpected token at ': *** [all] Error 1'  [0;32m~/autograde/hw0p2_test.py[0m in [0;36m&lt;module&gt;[0;34m[0m [1;32m     30[0m                                           collate_fn=ExampleDataset1.collate_fn) [0;31mValueError[0m: expected sequence of length 6 at dim 1 (got 4) make: *** [all] Error 1 The stack trace is as follow:   An error occurred while parsing the autoresult returned by the Autograder.           Error message: 783: unexpected token at ': *** [all] Error 1'  Autograder [Thu Aug 26 10:38:52 2021]: Received job 11785-f21_hw0p2_2_fuyut@andrew.cmu.edu:921 Autograder [Thu Aug 26 10:39:11 2021]: Success: Autodriver returned normally Autograder [Thu Aug 26 10:39:11 2021]: Here is the output from the autograder: --- Autodriver: Job exited with status 2 tar xvf autograde.tar autograde/ autograde/hw0p2_test.ipynb cp handin.ipynb autograde /usr/local/depot/anaconda3.7/bin/jupyter nbconvert --to script /home/autograde/autolab/autograde/handin.ipynb [NbConvertApp] Converting notebook /home/autograde/autolab/autograde/handin.ipynb to script [NbConvertApp] Writing 18350 bytes to /home/autograde/autolab/autograde/handin.py /usr/local/depot/anaconda3.7/bin/jupyter nbconvert --to script /home/autograde/autolab/autograde/hw0p2_test.ipynb [NbConvertApp] Converting notebook /home/autograde/autolab/autograde/hw0p2_test.ipynb to script [NbConvertApp] Writing 12144 bytes to /home/autograde/autolab/autograde/hw0p2_test.py /usr/local/depot/anaconda3.7/bin/ipython /home/autograde/autolab/autograde/hw0p2_test.py /bin/bash: nvidia-smi: command not found Batch 0 :  tensor([2, 3])   Batch 1 :  tensor([4, 5])   Batch 2 :  tensor([6, 7])   Batch 3 :  tensor([8, 9])   Batch 0 :  (tensor([2, 3]), tensor([4, 9]))   Batch 1 :  (tensor([4, 5]), tensor([16, 25]))   Batch 2 :  (tensor([6, 7]), tensor([36, 49]))   Batch 3 :  (tensor([8, 9]), tensor([64, 81]))   Batch 0 :  tensor([[ 2,  3,  4],         [ 4,  6,  8],         [ 6,  9, 12]])   Batch 1 :  tensor([[ 8, 12, 16],         [10, 15, 20],         [12, 18, 24]])   Batch 0 :  (tensor([[ 2,  3,  4],         [ 4,  6,  8],         [ 6,  9, 12]]), tensor([1, 2, 3]))   Batch 1 :  (tensor([[ 8, 12, 16],         [10, 15, 20],         [12, 18, 24]]), tensor([4, 5, 6]))   Batch 0 :  tensor([[[ 0,  0,  0],          [ 2,  3,  4],          [ 4,  6,  8]],          [[ 2,  3,  4],          [ 4,  6,  8],          [ 6,  9, 12]]])   Batch 1 :  tensor([[[ 4,  6,  8],          [ 6,  9, 12],          [ 8, 12, 16]],          [[ 6,  9, 12],          [ 8, 12, 16],          [ 0,  0,  0]]])   Batch 2 :  tensor([[[ 0,  0,  0],          [10, 15, 20],          [12, 18, 24]],          [[10, 15, 20],          [12, 18, 24],          [ 0,  0,  0]]])   Batch 0 :  (tensor([[[ 0,  0,  0],          [ 2,  3,  4],          [ 4,  6,  8]],          [[ 2,  3,  4],          [ 4,  6,  8],          [ 6,  9, 12]]]), tensor([1, 2]))   Batch 1 :  (tensor([[[ 4,  6,  8],          [ 6,  9, 12],          [ 8, 12, 16]],          [[ 6,  9, 12],          [ 8, 12, 16],          [ 0,  0,  0]]]), tensor([3, 4]))   Batch 2 :  (tensor([[[ 0,  0,  0],          [10, 15, 20],          [12, 18, 24]],          [[10, 15, 20],          [12, 18, 24],          [ 0,  0,  0]]]), tensor([5, 6]))   [0;31m---------------------------------------------------------------------------[0m [0;31mValueError[0m                                Traceback (most recent call last) [0;32m~/autograde/hw0p2_test.py[0m in [0;36m&lt;module&gt;[0;34m[0m [1;32m     30[0m                                           collate_fn=ExampleDataset1.collate_fn) [1;32m     31[0m [0;34m[0m[0m [0;32m---&gt; 32[0;31m [0;32mfor[0m [0mi[0m[0;34m,[0m [0mbatch[0m [0;32min[0m [0menumerate[0m[0;34m([0m[0mdataloader1[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m [0m[1;32m     33[0m     [0mprint[0m[0;34m([0m[0;34m\"\"Batch\"\"[0m[0;34m,[0m [0mi[0m[0;34m,[0m [0;34m\"\":\\n\"\"[0m[0;34m,[0m [0mbatch[0m[0;34m,[0m [0;34m\"\"\\n\"\"[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m [1;32m     34[0m [0;34m[0m[0m  [0;32m/usr/local/depot/anaconda3.7/lib/python3.7/site-packages/torch/utils/data/dataloader.py[0m in [0;36m__next__[0;34m(self)[0m [1;32m    344[0m     [0;32mdef[0m [0m__next__[0m[0;34m([0m[0mself[0m[0;34m)[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m [1;32m    345[0m         [0mindex[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0m_next_index[0m[0;34m([0m[0;34m)[0m  [0;31m# may raise StopIteration[0m[0;34m[0m[0;34m[0m[0m [0;32m--&gt; 346[0;31m         [0mdata[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mdataset_fetcher[0m[0;34m.[0m[0mfetch[0m[0;34m([0m[0mindex[0m[0;34m)[0m  [0;31m# may raise StopIteration[0m[0;34m[0m[0;34m[0m[0m [0m[1;32m    347[0m         [0;32mif[0m [0mself[0m[0;34m.[0m[0mpin_memory[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m [1;32m    348[0m             [0mdata[0m [0;34m=[0m [0m_utils[0m[0;34m.[0m[0mpin_memory[0m[0;34m.[0m[0mpin_memory[0m[0;34m([0m[0mdata[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m  [0;32m/usr/local/depot/anaconda3.7/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py[0m in [0;36mfetch[0;34m(self, possibly_batched_index)[0m [1;32m     45[0m         [0;32melse[0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m [1;32m     46[0m             [0mdata[0m [0;34m=[0m [0mself[0m[0;34m.[0m[0mdataset[0m[0;34m[[0m[0mpossibly_batched_index[0m[0;34m][0m[0;34m[0m[0;34m[0m[0m [0;32m---&gt; 47[0;31m         [0;32mreturn[0m [0mself[0m[0;34m.[0m[0mcollate_fn[0m[0;34m([0m[0mdata[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m [0m [0;32m~/autograde/handin.py[0m in [0;36mcollate_fn[0;34m(batch)[0m [1;32m     55[0m [0;34m[0m[0m [1;32m     56[0m         [0;31m### Convert batch to tensor (1 line)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m [0;32m---&gt; 57[0;31m         [0mbatch_x[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mas_tensor[0m[0;34m([0m[0mbatch[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m [0m[1;32m     58[0m [0;34m[0m[0m [1;32m     59[0m         [0;31m### Return batched data and labels (1 line)[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m  [0;31mValueError[0m: expected sequence of length 6 at dim 1 (got 4) make: *** [all] Error 1  Score for this problem: 0.0  Graded by:    It seems like there are something wrong with my exercise 1. Here is my snippet:  class ExampleDataset1(torch.utils.data.Dataset):          def __init__(self, X):                  ### Assign data to self (1 line)         self.X = X                  ### Assign length to self (1 line)         self.length = len(X)              def __len__(self):                  ### Return length (1 line)         return self.length          def __getitem__(self, i):                  ### Return data at index i (1 line)         return X[i]          def collate_fn(batch):                  ### Convert batch to tensor (1 line)         batch_x = torch.as_tensor(batch)                  ### Return batched data and labels (1 line)         return batch_x I am using Jupyter Lab and got all expected output local. How can I fix this? Thanks a lot.\"\n",
            "Top 22 A Hi,   Your collate function should be returning both X and y batches but you are returning only batch_x.   Fix that and see what results.\n",
            "Top 23 Q Hi, I'm getting 0 for my hw0p1 notebook on Autolab but when I run the on Colab, I do get the desired output. I have also restarted and run all cells as said by one of the instructor. This is for every function I wrote and not just a particular function https://colab.research.google.com/drive/11gW2g9S-J1FqVuCaYFOyCr_RyUMePrqW?usp=sharing  Can someone please have a look at my notebook and see if I'm doing anything wrong?   For every part, I'm getting the same error\n",
            "Top 23 A \"To start off, I would suggest the following changes: Change \"\"NotImplemented\"\" to some other . This is an inbuilt constant in Python and shouldn't be used unless you really intend to,Reading the \"\"AttributeError: module 'torch' has no attribute 'multiply'\"\". You should read the PyTorch docs to find if there is another PyTorch function that will let you do the same operation. Try using that instead.\"\n",
            "Top 24 Q \"Hello, I've submitted my p2, and although I scored well locally, the autograder gave me all 0 (even for the non-torch questions; for reference, i am using numPy version 1.19.1). I noticed that in a similar post you asked us to check our autolab output, so I have pasted the first problem's output below. What is going on? I'm pretty lost.   An error occurred while parsing the autoresult returned by the Autograder.           Error message: 783: unexpected token at ': *** [all] Error 1'  Autograder [Fri Sep  4 19:22:11 2020]: Received job 11485-f20_hw0_2_jmasom@andrew.cmu.edu:603 Autograder [Fri Sep  4 19:22:29 2020]: Success: Autodriver returned normally Autograder [Fri Sep  4 19:22:29 2020]: Here is the output from the autograder: --- Autodriver: Job exited with status 2 tar xvf autograde.tar ./._autograde autograde/ autograde/._hw0_test.ipynb autograde/hw0_test.ipynb cp handin.c autograde/handin.ipynb /usr/local/depot/anaconda3.7/bin/jupyter nbconvert --to script /home/autograde/autolab/autograde/handin.ipynb [NbConvertApp] Converting notebook /home/autograde/autolab/autograde/handin.ipynb to script [NbConvertApp] Writing 51431 bytes to /home/autograde/autolab/autograde/handin.py /usr/local/depot/anaconda3.7/bin/jupyter nbconvert --to script /home/autograde/autolab/autograde/hw0_test.ipynb [NbConvertApp] Converting notebook /home/autograde/autolab/autograde/hw0_test.ipynb to script [NbConvertApp] Writing 26014 bytes to /home/autograde/autolab/autograde/hw0_test.py /usr/local/depot/anaconda3.7/bin/ipython /home/autograde/autolab/autograde/hw0_test.py 7082791 [[  59092 -144096  136512 ...  -53088  -86268   53404]  [  82467 -201096  190512 ...  -74088 -120393   74529]  [-122111  297768 -282096 ...  109704  178269 -110357]  ...  [-144551  352488 -333936 ...  129864  211029 -130637]  [-179707  438216 -415152 ...  161448  262353 -162409]  [  88825 -216600  205200 ...  -79800 -129675   80275]] [  59092 -201096 -282096 ...  129864  262353   80275] 265421520 [[  0   0 653 ... 773 961   0]  [  0 456   0 ... 168 273   0]  [936 475   0 ... 408   0   0]  ...  [  0 396 457 ... 646   0   0]  [645 943   0 ... 863   0 790]  [641   0 379 ... 347   0   0]] [[0 0 1 ... 1 1 0]  [0 1 0 ... 1 1 0]  [1 1 0 ... 1 0 0]  ...  [0 1 1 ... 1 0 0]  [1 1 0 ... 1 0 1]  [1 0 1 ... 1 0 0]] [[[5. 9. 2. 6.]   [5. 3. 5. 8.]]   [[9. 7. 9. 3.]   [2. 3. 8. 4.]]   [[2. 7. 9. 5.]   [0. 2. 8. 8.]]] [[7. 2. 7. 1. 6. 5. 0. 0. 3. 1. 9. 9. 6. 6. 7. 8. 8. 7. 0. 8. 6. 8. 9. 8.   3. 6. 1. 7. 4. 9. 2. 0. 8. 2. 7. 8. 4. 4. 1. 7.]  [6. 9. 4. 1. 5. 9. 7. 1. 3. 5. 7. 3. 6. 6. 7. 9. 1. 9. 6. 0. 3. 8. 4. 1.   4. 5. 0. 3. 1. 4. 4. 4. 0. 0. 8. 4. 6. 9. 3. 3.]  [2. 1. 2. 1. 3. 4. 1. 1. 0. 7. 8. 4. 3. 5. 6. 3. 2. 9. 8. 1. 4. 0. 8. 3.   9. 5. 5. 1. 7. 8. 6. 4. 7. 3. 5. 3. 6. 4. 7. 3.]  [0. 5. 9. 3. 7. 5. 5. 8. 0. 8. 3. 6. 9. 3. 2. 7. 0. 3. 0. 3. 6. 1. 9. 2.   9. 4. 9. 1. 3. 2. 4. 9. 7. 4. 9. 4. 1. 2. 7. 2.]  [3. 9. 7. 6. 6. 2. 3. 6. 0. 8. 0. 7. 6. 5. 9. 6. 5. 2. 7. 1. 9. 2. 2. 5.   6. 4. 2. 2. 1. 0. 9. 0. 2. 8. 3. 0. 8. 8. 1. 0.]] [[[3. 1. 4. 1.]   [5. 9. 2. 6.]]   [[9. 7. 9. 3.]   [2. 3. 8. 4.]]   [[6. 2. 6. 4.]   [3. 3. 8. 3.]]] [[8. 7. 0. 3. 8. 7. 7. 1. 8. 4. 7. 0. 4. 9. 0. 6. 4. 2. 4. 6. 3. 3. 7. 8.   5. 0. 8. 5. 4. 7. 4. 1. 3. 3. 9. 2. 5. 2. 3. 5.]  [7. 2. 7. 1. 6. 5. 0. 0. 3. 1. 9. 9. 6. 6. 7. 8. 8. 7. 0. 8. 6. 8. 9. 8.   3. 6. 1. 7. 4. 9. 2. 0. 8. 2. 7. 8. 4. 4. 1. 7.]  [6. 9. 4. 1. 5. 9. 7. 1. 3. 5. 7. 3. 6. 6. 7. 9. 1. 9. 6. 0. 3. 8. 4. 1.   4. 5. 0. 3. 1. 4. 4. 4. 0. 0. 8. 4. 6. 9. 3. 3.]] [[[5. 9. 2. 6.]   [5. 3. 5. 8.]]   [[9. 7. 9. 3.]   [2. 3. 8. 4.]]   [[6. 2. 6. 4.]   [3. 3. 8. 3.]]] [[3. 3. 7. 9. 9. 9. 7. 3. 2. 3. 9. 7. 7. 5. 1. 2. 2. 8. 1. 5. 8. 4. 0. 2.   5. 5. 0. 8. 1. 1. 0. 3. 8. 8. 4. 4. 0. 9. 3. 7.]  [3. 2. 1. 1. 2. 1. 4. 2. 5. 5. 5. 2. 5. 7. 7. 6. 1. 6. 7. 2. 3. 1. 9. 5.   9. 9. 2. 0. 9. 1. 9. 0. 6. 0. 4. 8. 4. 3. 3. 8.]  [8. 7. 0. 3. 8. 7. 7. 1. 8. 4. 7. 0. 4. 9. 0. 6. 4. 2. 4. 6. 3. 3. 7. 8.   5. 0. 8. 5. 4. 7. 4. 1. 3. 3. 9. 2. 5. 2. 3. 5.]  [7. 2. 7. 1. 6. 5. 0. 0. 3. 1. 9. 9. 6. 6. 7. 8. 8. 7. 0. 8. 6. 8. 9. 8.   3. 6. 1. 7. 4. 9. 2. 0. 8. 2. 7. 8. 4. 4. 1. 7.]] [[[3. 1. 4. 1.]   [5. 9. 2. 6.]   [5. 3. 5. 8.]   [5. 3. 5. 8.]]   [[9. 7. 9. 3.]   [2. 3. 8. 4.]   [2. 3. 8. 4.]   [9. 7. 9. 3.]]   [[6. 2. 6. 4.]   [3. 3. 8. 3.]   [2. 7. 9. 5.]   [0. 2. 8. 8.]]] [[8. 2. 4. 3. 1. 6. 5. 8. 4. 3. 6. 5. 3. 7. 8. 8. 3. 7. 8. 5. 7. 2. 7. 8.   0. 7. 4. 8. 4. 4. 0. 4. 8. 0. 0. 4. 7. 3. 7. 7.]  [2. 2. 1. 7. 0. 7. 5. 9. 7. 1. 1. 2. 4. 1. 4. 5. 8. 2. 1. 6. 3. 0. 3. 9.   5. 1. 3. 7. 1. 1. 7. 9. 4. 2. 0. 3. 2. 4. 0. 0.]  [9. 3. 8. 3. 0. 4. 4. 0. 2. 5. 5. 8. 2. 7. 3. 6. 1. 0. 2. 2. 5. 5. 1. 2.   8. 7. 3. 7. 3. 1. 0. 1. 0. 8. 8. 5. 3. 3. 1. 0.]  [6. 1. 6. 9. 5. 7. 0. 1. 4. 9. 5. 1. 6. 5. 4. 4. 4. 7. 2. 2. 6. 5. 3. 0.   8. 8. 1. 8. 7. 5. 7. 9. 4. 0. 7. 2. 3. 9. 5. 4.]  [0. 4. 5. 8. 1. 4. 8. 0. 1. 1. 8. 9. 4. 9. 0. 3. 0. 7. 0. 8. 1. 2. 8. 5.   8. 2. 1. 3. 5. 0. 2. 5. 8. 6. 2. 7. 7. 1. 8. 4.]  [9. 3. 3. 2. 9. 0. 4. 6. 4. 3. 2. 3. 1. 1. 2. 7. 2. 7. 0. 1. 8. 0. 5. 2.   8. 0. 4. 0. 3. 8. 1. 6. 4. 6. 9. 6. 4. 7. 2. 9.]  [9. 3. 3. 2. 9. 0. 4. 6. 4. 3. 2. 3. 1. 1. 2. 7. 2. 7. 0. 1. 8. 0. 5. 2.   8. 0. 4. 0. 3. 8. 1. 6. 4. 6. 9. 6. 4. 7. 2. 9.]  [0. 4. 5. 8. 1. 4. 8. 0. 1. 1. 8. 9. 4. 9. 0. 3. 0. 7. 0. 8. 1. 2. 8. 5.   8. 2. 1. 3. 5. 0. 2. 5. 8. 6. 2. 7. 7. 1. 8. 4.]] [[[3. 1. 4. 1.]   [5. 9. 2. 6.]   [5. 3. 5. 8.]   [0. 0. 0. 0.]]   [[0. 0. 0. 0.]   [9. 7. 9. 3.]   [2. 3. 8. 4.]   [0. 0. 0. 0.]]   [[6. 2. 6. 4.]   [3. 3. 8. 3.]   [2. 7. 9. 5.]   [0. 2. 8. 8.]]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]  [3. 5. 2. 4. 7. 6. 8. 8. 1. 6. 7. 7. 8. 1. 5. 9. 8. 9. 4. 3. 0. 3. 5. 0.   2. 3. 8. 1. 3. 3. 3. 7. 0. 1. 9. 9. 0. 4. 7. 3.]  [2. 7. 2. 0. 0. 4. 5. 5. 6. 8. 4. 1. 4. 9. 8. 1. 1. 7. 9. 9. 3. 6. 7. 2.   0. 3. 5. 9. 4. 4. 6. 4. 4. 3. 4. 4. 8. 4. 3. 7.]  [5. 5. 0. 1. 5. 9. 3. 0. 5. 0. 1. 2. 4. 2. 0. 3. 2. 0. 7. 5. 9. 0. 2. 7.   2. 9. 2. 3. 3. 2. 3. 4. 1. 2. 9. 1. 4. 6. 8. 2.]  [3. 0. 0. 6. 0. 6. 3. 3. 8. 8. 8. 2. 3. 2. 0. 8. 8. 3. 8. 2. 8. 4. 3. 0.   4. 3. 6. 9. 8. 0. 8. 5. 9. 0. 9. 6. 5. 3. 1. 8.]  [0. 4. 9. 6. 5. 7. 8. 8. 9. 2. 8. 6. 6. 9. 1. 6. 8. 8. 3. 2. 3. 6. 3. 6.   5. 7. 0. 8. 4. 6. 5. 8. 2. 3. 9. 7. 5. 3. 4. 5.]  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.   0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] &lt;class 'torch.Tensor'&gt; &lt;class 'numpy.ndarray'&gt; tensor(265421520) [0;31m---------------------------------------------------------------------------[0m [0;31mRuntimeError[0m                              Traceback (most recent call last) [0;32m~/autograde/hw0_test.py[0m in [0;36m&lt;module&gt;[0;34m[0m [1;32m     16[0m [0;34m[0m[0m [1;32m     17[0m [0;31m# From handin.ipynp -&gt; handin.py[0m[0;34m[0m[0;34m[0m[0;34m[0m[0m [0;32m---&gt; 18[0;31m [0;32mfrom[0m [0mhandin[0m [0;32mimport[0m [0;34m*[0m[0;34m[0m[0;34m[0m[0m [0m[1;32m     19[0m [0;34m[0m[0m [1;32m     20[0m [0;34m[0m[0m  [0;32m~/autograde/handin.py[0m in [0;36m&lt;module&gt;[0;34m[0m [1;32m   1435[0m [0mX[0m [0;34m=[0m [0mtorch[0m[0;34m.[0m[0mfrom_numpy[0m[0;34m([0m[0mX[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m [1;32m   1436[0m [0;34m[0m[0m [0;32m-&gt; 1437[0;31m [0mprint[0m[0;34m([0m[0mtensor_ReLU[0m[0;34m([0m[0mX[0m[0;34m)[0m[0;34m)[0m[0;34m[0m[0;34m[0m[0m [0m[1;32m   1438[0m [0;34m[0m[0m [1;32m   1439[0m [0;34m[0m[0m  [0;32m~/autograde/handin.py[0m in [0;36mtensor_ReLU[0;34m(x)[0m [1;32m   1423[0m     \"\"\"\"\"\" [1;32m   1424[0m     [0mmask[0m [0;34m=[0m [0mx[0m [0;34m&gt;[0m [0;36m0[0m[0;34m[0m[0;34m[0m[0m [0;32m-&gt; 1425[0;31m     [0;32mreturn[0m [0mx[0m [0;34m*[0m [0mmask[0m[0;34m[0m[0;34m[0m[0m [0m[1;32m   1426[0m [0;34m[0m[0m [1;32m   1427[0m [0;34m[0m[0m  [0;31mRuntimeError[0m: expected device cpu and dtype Long but got device cpu and dtype Bool make: *** [all] Error 1  Score for this problem: 0.0  Graded by:\"\n",
            "Top 24 A \"It looks like you are failing 4.4, can you \"\"pass\"\" 4.4 and 4.5, and upload you answers again and see if you are scored. If you get a score you need to fix your solutions to 4.4 and/or 4.5. Autolab parses jupyter notebooks in a way where if one of your answer is wrong, you will get 0s in all.\"\n",
            "Top 25 Q Hi,I worked on HW0P1 in google colab notebook. I got the answers for questions 1 and 2 which matches the expected results. But when I upload that in autolab I am getting my scores as zero.I need help on solving this.I’ve used a functions like torch.from_numpy()X.numpy()np.dot(x,y)etc.Isn’t that correct way of doing the assignment?I am completely new to python, just started learning now. Could you please help me with this?\n",
            "Top 25 A Can you show exactly what error you got on autolab?\n",
            "Top 26 Q for  class ExampleDataset1(torch.utils.data.Dataset): the function  collate_fn(batch): it ask us to return batched data and labels but we only have X here, what does the label refer to here? Thanks    \n",
            "Top 26 A You just have to return batch_x\n",
            "Top 27 Q Hello, I got a ValueError when the autograder runs my jupyter notebook, but the notebook worked fine without error when I ran it locally. Any suggestions on what might be the issue? Autograder submission link: https://autolab.andrew.cmu.edu/courses/11785-f22/assessments/hw0p2/viewFeedback?feedback=87870&amp;submission_id=8087996\n",
            "Top 27 A This might be due to a code error. Are you sure your code is okay? Yday someone faced the same error and it was because they were returning X instead of self.X in get item\n",
            "Top 28 Q \"Good afternoon Instructors,   I hope you're all doing well. I'm unsure of what mistake I've made with my submission however it's obvious that the autograder does not like what I am doing.   I receive the following error message on all of my questions:    An error occurred while parsing the autoresult returned by the Autograder.           Error message: 783: unexpected token at ': *** [autograde] Error 1'  Autograder [Thu Jan 13 18:14:05 2022]: Received job 11785-s22_hw0p1_4_ibankole@andrew.cmu.edu:227 Autograder [Thu Jan 13 18:14:24 2022]: Success: Autodriver returned normally Autograder [Thu Jan 13 18:14:24 2022]: Here is the output from the autograder: --- Autodriver: Job exited with status 2 tar xvf autograde.tar autograde/ autograde/._grade.py autograde/grade.py cp handin.ipynb autograde /usr/local/depot/anaconda3.7/bin/jupyter nbconvert --to script /home/autograde/autolab/autograde/handin.ipynb [NbConvertApp] Converting notebook /home/autograde/autolab/autograde/handin.ipynb to script [NbConvertApp] Writing 110567 bytes to /home/autograde/autolab/autograde/handin.py /usr/local/depot/anaconda3.7/bin/python3 /home/autograde/autolab/autograde/grade.py 1.16.4 1.2.0 2 1 &lt;class 'torch.Tensor'&gt; &lt;class 'numpy.ndarray'&gt; 7082791 tensor(7082791) [[  59092 -144096  136512 ...  -53088  -86268   53404]  [  82467 -201096  190512 ...  -74088 -120393   74529]  [-122111  297768 -282096 ...  109704  178269 -110357]  ...  [-144551  352488 -333936 ...  129864  211029 -130637]  [-179707  438216 -415152 ...  161448  262353 -162409]  [  88825 -216600  205200 ...  -79800 -129675   80275]] tensor([[  59092, -144096,  136512,  ...,  -53088,  -86268,   53404],         [  82467, -201096,  190512,  ...,  -74088, -120393,   74529],         [-122111,  297768, -282096,  ...,  109704,  178269, -110357],         ...,         [-144551,  352488, -333936,  ...,  129864,  211029, -130637],         [-179707,  438216, -415152,  ...,  161448,  262353, -162409],         [  88825, -216600,  205200,  ...,  -79800, -129675,   80275]]) [  59092 -201096 -282096 ...  129864  262353   80275] tensor([  59092, -201096, -282096,  ...,  129864,  262353,   80275]) 265421520 tensor(265421520) [[  0   0 653 ... 773 961   0]  [  0 456   0 ... 168 273   0]  [936 475   0 ... 408   0   0]  ...  [  0 396 457 ... 646   0   0]  [645 943   0 ... 863   0 790]  [641   0 379 ... 347   0   0]] Traceback (most recent call last):   File \"\"/home/autograde/autolab/autograde/grade.py\"\", line 19, in &lt;module&gt;     from handin import *   File \"\"/home/autograde/autolab/autograde/handin.py\"\", line 739, in &lt;module&gt;     print(PYTORCH_ReLU(X))   File \"\"/home/autograde/autolab/autograde/handin.py\"\", line 725, in PYTORCH_ReLU     return (x&gt;0)*x RuntimeError: expected device cpu and dtype Long but got device cpu and dtype Bool make: *** [autograde] Error 1  Score for this problem: 0.0  Graded by:    I believe the most important portion of this excerpt is towards the end    File \"\"/home/autograde/autolab/autograde/handin.py\"\", line 724, in PYTORCH_ReLU     return (x&gt;0)*x RuntimeError: expected device cpu and dtype Long but got device cpu and dtype Bool      It seems the problem is that the auto grader does not want me to filter, since (x&gt;0) returns an array of Bool? My impression is that this is a very common technique when using numpy and pytorch, so I believe i'm making some other mistake.    This is the code I've written for it:    def PYTORCH_ReLU(x):     \"\"\"\"\"\"     Applies the rectified linear unit function element-wise.      Parameters:      x (torch.Tensor): 2-dimensional torch tensor.      Returns:      torch.Tensor: 2-dimensional torch tensor.     \"\"\"\"\"\"      return (x&gt;0)*x Could someone please help me understand what I've done wrong? All of the code produces the expected results in the notebook   \"\n",
            "Top 28 A You can try x * (x &gt; 0).type(torch.int64) since it's a pytorch tensor\n",
            "Top 29 Q Hi, It claims in the handout that we will be evaluated using torch==1.7.0 but I observed that autolab actually using 1.2.0. so I got following error:   Should I do the homework using torch 1.2.0 and numpy 1.6.5 then?\n",
            "Top 29 A Unfortunately yes, but only for part 1s. For part 2, you'll use your own environment\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dist_matrix.shape)\n",
        "print(dev_indices[:3], dev_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_J5opeDNlhZ",
        "outputId": "927af88b-f15b-48db-bbbb-8e7bfed1ae85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7, 336)\n",
            "[[87], [216], [6]] ['When the value is &gt;= 0, it should be 1', 'Yes', 'Could you please post a picture, there is absolute no reference for this question :-)', 'Post a screenshot of your code, while I guess the reason is your x is actually an 1-d tensor', 'AutoLab will force your file name to be what we specify internally, which is handin.ipynb for hw0p1 and handin.tar for hw0p2.  In other words, you can submit whatever file name you want but you want to make sure you submit the correct file type.', 'Not yet.', 'You can use whatever region makes sense for you. I use us-west-2 because I am on the west coast.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"/content/gdrive/MyDrive/Sem2/IDL/20s_gpt_eval_filtered (1).json\") as f:\n",
        "    sample_json_list = json.load(f)\n",
        "\n",
        "data_dict = { dev_corpus[i] : dev_labels[i] for i in range(len(data)) }\n",
        "sample = [d for d in sample_json_list if task_name in d[\"label\"] and d[\"num_examples\"] == 15]\n",
        "\n",
        "for d in sample_json_list:\n",
        "  if task_name in d[\"label\"] and d[\"num_examples\"] == 15:\n",
        "    q = d['question']\n",
        "    print(\"Q:\", d[\"label\"], q)\n",
        "    if q in data_dict: \n",
        "      print(\"A:\", data_dict[q])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5lapHOeXo46",
        "outputId": "77deb94c-d960-413a-c453-6951f61bf39c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: hw0 Hi,   I wasn't sure if this is giving away any answers, so I made this private. I'm using np.vectorize (doc linked below) for the hw and it's taking around 2-3 seconds to actually run for the given test case. Is this reasonable? If not, can you guys give me a hint?   https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html  Thank you!\n",
            "Q: hw0 I have used several ways to implement the function. I think they look right on my notebook, but it just cannot pass Autolab:  answer1:  return torch.where(x&gt;0,torch.ones_like(x),torch.zeros_like(x)) answer2: return torch.gt(x,0).long() Could anyone take a look? Thanks!\n",
            "A: When the value is &gt;= 0, it should be 1\n",
            "Q: hw0 Hi, I was trying to complete hw0 but got stuck on 3.3. In this question, if I understand it correctly, it is asking us to slice the second dimension to d. If the offset is 0, the slicing is from the start of the array and if the offset is not 0 the slicing is from the offset index. In this case, for test example 1,  the offset is [1,0,0] and d is 2 so it means that the slicing on the first array is from index 1 while the slicing on the second and third array is from index 0 and the resulting sliced pieces have a length of 2. I tried to use list comprehension like previous two questions (3.1 and 3.2) but I don't know how to handle the different cases for these arrays. Could you give me some hint on how to do this? Or is there something I understand incorrectly for this question? Thank you.\n",
            "Q: hw0 I tried the same thing in documents, but it still occurs permission issue. I used Cygwin with Windows10. I can't access ec2 in AWS. Did anybody have the same issue and resolves this?    xxxx@xxxx /cygdrive/c/deeplearning$ ssh -i deeplearning.pem -L 8000:localhost:8888 ubuntu@-------.compute-1.amazonaws.com  The authenticity of host '-------.compute-1.amazonaws.com (18.207.205.40)' can't be established.ECDSA key fingerprint is SHA256:32XV7Oc3DvxSr8JggwCYWouUbSvopFm5XWtnUY94FBU.Are you sure you want to continue connecting (yes/no)? yesWarning: Permanently added '-------.compute-1.amazonaws.com,18.207.205.40' (ECDSA) to the list of known hosts.@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: UNPROTECTED PRIVATE KEY FILE! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@Permissions for 'deeplearning.pem' are too open.It is required that your private key files are NOT accessible by others.This private key will be ignored.Load key \"deeplearning.pem\": bad permissionsubuntu@-------.compute-1.amazonaws.com: Permission denied (publickey). xxxx@xxxx /cygdrive/c/deeplearning$ chmod 400 deeplearning.pem  xxxx@xxxx /cygdrive/c/deeplearning$ ssh -i deeplearning.pem -L 8000:localhost:8888 ubuntu@-------.compute-1.amazonaws.com  @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: UNPROTECTED PRIVATE KEY FILE! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@Permissions for 'deeplearning.pem' are too open.It is required that your private key files are NOT accessible by others.This private key will be ignored.Load key \"deeplearning.pem\": bad permissionsubuntu@-------.compute-1.amazonaws.com: Permission denied (publickey).  \n",
            "Q: hw0 Once I create a new tensor, I will add \"dtype=torch.int32\" and I will also get a suffix \"dtype=torch.int32\" in the result. Does it matter or what is the purpose of \"dtype=torch.int32\"？ \n",
            "Q: hw0 Are we allowed to use for loops after the vectorization section of the homework?   Also, if we can use for loops, do we have to use list comprehensions, or can we still use \"for _ in __:\"?\n",
            "Q: hw0 For implementing functions like outer, are we allowed to use the numpy functions available\n",
            "A: Yes\n",
            "Q: hw0  Getting the above error and not able to download the data file.\n",
            "Q: hw0 Hi there,  I am trying to launch g4dn.xlarge, but it shows like this:   I think it is a problem of service limit. So I go to request for more service on the support webpage. It shows me that there are many choices of instances I can request:   Which item should I choose to request, in order to launch g4dn.large?  Thanks!\n",
            "Q: hw0 I am waiting for 2 days, but I still did not get the limit increased. This morning I got an email saying that the service team has not responded and they will give me an update as soon as possible. Is there anyway I can get access to the ec2 instance? \n",
            "Q: hw0 Hi, I am planning to use the Lambda system for the course instead of AWS.  I have cuda, torch and numpy installed/configured. I haven't installed conda as I prefer to use pip instead. Below you can find the output of 'numba -s' command which I have submitted in Autolab.  Please let me know if this is sufficient or should I have to configure the AWS.  Note : I have created a AWS account with GPU access and plan on using it only if I find the Lambda system insufficient.  (xx_python_venv) xxxx@xxx:~/xxx$ numba -sSystem info:--------------------------------------------------------------------------------__Time Stamp__2020-01-15 22:36:28.987988 __Hardware Information__Machine : x86_64CPU Name : znver1CPU count : 32CFS restrictions : NoneCPU Features : 64bit adx aes avx avx2 bmi bmi2 clflushopt clzero cmov cx16 f16c fma fsgsbaselzcnt mmx movbe mwaitx pclmul popcnt prfchw rdrnd rdseed sahf sha sse sse2 sse3sse4.1 sse4.2 sse4a ssse3 xsave xsavec xsaveopt xsaves __OS Information__Platform : Linux-5.0.0-37-generic-x86_64-with-Ubuntu-18.04-bionicRelease : 5.0.0-37-genericSystem Name : LinuxVersion : #40~18.04.1-Ubuntu SMP Thu Nov 14 12:06:39 UTC 2019OS specific info : Ubuntu18.04bionicglibc info : glibc 2.25 __Python Information__Python Compiler : GCC 8.3.0Python Implementation : CPythonPython Version : 3.6.8Python Locale : en_US UTF-8 __LLVM information__LLVM version : 8.0.0 __CUDA Information__Found 1 CUDA devicesid 0 b'TITAN RTX' [SUPPORTED] compute capability: 7.5 pci device id: 0 pci bus id: 66Summary: 1/1 devices are supportedCUDA driver version : 10010CUDA libraries:Finding cublas from &lt;unavailable&gt; ERROR: can't locate libFinding cusparse from &lt;unavailable&gt; ERROR: can't locate libFinding cufft from &lt;unavailable&gt; ERROR: can't locate libFinding curand from &lt;unavailable&gt; ERROR: can't locate libFinding nvvm from &lt;unavailable&gt; ERROR: can't locate libFinding libdevice from &lt;unavailable&gt; searching for compute_20... ERROR: can't open libdevice for compute_20 searching for compute_30... ERROR: can't open libdevice for compute_30 searching for compute_35... ERROR: can't open libdevice for compute_35 searching for compute_50... ERROR: can't open libdevice for compute_50 __ROC Information__ROC available : FalseError initialising ROC due to : No ROC toolchains found.No HSA Agents found, encountered exception when searching:Error at driver init: NUMBA_HSA_DRIVER /opt/rocm/lib/libhsa-runtime64.so is not a valid file path. Note it must be a filepath of the .so/.dll/.dylib or the driver: __SVML Information__SVML state, config.USING_SVML : FalseSVML library found and loaded : Falsellvmlite using SVML patched LLVM : TrueSVML operational : False __Threading Layer Information__TBB Threading layer available : TrueOpenMP Threading layer available : False+--&gt; Disabled due to : Unknown import problem.Workqueue Threading layer available : True __Numba Environment Variable Information__None set. __Conda Information__Conda not present/not working.Error was [Errno 2] No such file or directory: 'conda': 'conda' --------------------------------------------------------------------------------If requested, please copy and paste the information betweenthe dashed (----) lines, or from a given specific section asappropriate. =============================================================IMPORTANT: Please ensure that you are happy with sharing thecontents of the information present, any information that youwish to keep private you should remove before sharing.============================================================= (xx_python_venv) xxxx@xxx:~/xxx$ python3Python 3.6.8 (default, Oct 7 2019, 12:59:55) [GCC 8.3.0] on linuxType \"help\", \"copyright\", \"credits\" or \"license\" for more information.&gt;&gt;&gt; import torch&gt;&gt;&gt; import numpy&gt;&gt;&gt; \n",
            "Q: hw0 Hi, I wonder why when I run the following, my Python just crashes. Any hints will be appreciated. Thanks! \n",
            "Q: hw0 Hi! I have a quick question about HW0p1 numbers 2.6 and 4.5. I'm not sure why they're being marked wrong on AutoLab. My code for 2.6 is below. My code for 4.5 is nearly identical.  def PrimeReLU(x):     \"\"\"     Applies the derivative of the rectified linear unit function      element-wise.      Parameters:      x (numpy.ndarray): 2-dimensional numpy array.      Returns:      numpy.ndarray: 2-dimensional numpy array.     \"\"\"     result = np.copy(x)     result[result &lt; 0] = 0     result[result &gt; 0] = 1          return result  The output of this function matches the output provided in the notebook; however, AutoLab is marking it wrong. Am I missing something?  Thanks for your help!\n",
            "Q: hw0  I am launch a failed message when I trying to launch a new ec2 instance saying that my vCPU limit is 0 but I had requested for increase in service limit and it has been approved. I had requested for All G instances and am trying to launch g4dn.xLarge. \n",
            "Q: hw0 Is it possible to fetch the last m points of an ndarray (implement slice last point) without using a for loop?? Thanks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_json_list\n",
        "# save data to tsv question, answer, each label each"
      ],
      "metadata": {
        "id": "SEXSqMy8sWvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [\"hw0\", 'hw1', 'hw1p1', 'hw1p2', 'hw2', 'hw2p1', 'hw2p2', \n",
        "          'hw3', 'hw3p1', 'hw3p2', 'hw4', 'hw4p1', 'hw4p2', \n",
        "          'project', 'quiz', 'quizzes']"
      ],
      "metadata": {
        "id": "94xyjdw-rl94"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}